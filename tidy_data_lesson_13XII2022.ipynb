{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy Data\n",
    "\n",
    "> Leo Tolstoy said \"Happy families are all alike; every unhappy family is unhappy in its own way.\"<br />\n",
    "> \"Like families,\" Hadley Wickham said in his [Tidy Data](http://vita.had.co.nz/papers/tidy-data.html) Paper, \"tidy datasets are all alike but every messy dataset is messy in its own way.\"<br />\n",
    "\n",
    "\n",
    "**Key components of tidy data**\n",
    "\n",
    "| Component | Why Necessary |\n",
    "|:-----------|:---------------|\n",
    "| Data is tabular | Exploration, Modeling |\n",
    "| One value per cell | Exploration, Modeling |\n",
    "| Each observation is one and only one row | Exploration, Modeling |\n",
    "| Each variable is one and only one column | Exploration |\n",
    "\n",
    "If a dataset does not meet these requirements, then it is an untidy dataset. \n",
    "\n",
    "We will now examine the 4 cases of untidy data. \n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Data is Tabular\n",
    "\n",
    "Tabular data is needed for exploration and modeling. \n",
    "\n",
    "An example of non-tabular data would be the text file, \"curriculum_access.txt\", that contains the following access logs of the web development curriculum:\n",
    "\n",
    "```\n",
    "2018-03-06 13:35:21 html-css/forms 132 23 97.105.19.61\n",
    "2018-03-06 13:35:23 slides/inheritance_and_polymorphism 29 22 97.105.19.61\n",
    "2018-03-06 13:35:41 html-css/forms 130 23 97.105.19.61\n",
    "2018-03-06 13:36:14 java-ii 19 22 97.105.19.61\n",
    "2018-03-06 13:36:16 java-ii/inheritance-and-polymorphism 19 22 97.105.19.61\n",
    "2018-03-06 13:36:17 slides/inheritance_and_polymorphism 19 22 97.105.19.61\n",
    "2018-03-06 13:36:19 java-ii/inheritance-and-polymorphism 35 22 97.105.19.61\n",
    "```\n",
    "\n",
    "To make this tabular, we can use string methods that will split this data into columns that represent the various variables that exist in this data. \n",
    "\n",
    "\n",
    "Key Componants Of Tidy Data\n",
    "\n",
    "- Tidy data is tabular (in a table-like structure).    \n",
    "- It has 1 observation per cell.  \n",
    "- Each variable is 1 and only 1 column.  (Exception : dummy variables for species, bc we cannot feed, typically, text into an algorithm. We turned 1 column into 3 Boolean columns, expanding the width of the dataset. This is not tidy, and, thus, is not useful for exploring, but it's useful for, later on, modelling.)\n",
    "- Each observation is 1 and only 1 row.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the dataset does not meet these requirements, it's 'untidy'.\n",
    "\n",
    "1 / Tabular data is needed for exploring and modelling. Untidy data can be made tidy and tabular using string manipulation, turning it into a series and split at the white space. For example.\n",
    "\n",
    "2 / One observation per cell :\n",
    "\n",
    "-- split the data into multiple columns untidy_df.City_State_Zip.str.split(pat = ',', expand = True)\n",
    "--untidy_df.drop(columns = ['City_State_Zip])\n",
    "\n",
    "-- rename the columns new_vars.columns = ['City', 'State', 'ZIP']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import env\n",
    "\n",
    "logs_list = ['2018-03-06 13:35:21 html-css/forms 132 23 97.105.19.61', \n",
    "             '2018-03-06 13:35:23 slides/inheritance_and_polymorphism 29 22 97.105.19.61', \n",
    "             '2018-03-06 13:35:41 html-css/forms 130 23 97.105.19.61',\n",
    "             '2018-03-06 13:36:14 java-ii 19 22 97.105.19.61',\n",
    "             '2018-03-06 13:36:16 java-ii/inheritance-and-polymorphism 19 22 97.105.19.61',\n",
    "             '2018-03-06 13:36:17 slides/inheritance_and_polymorphism 19 22 97.105.19.61',\n",
    "             '2018-03-06 13:36:19 java-ii/inheritance-and-polymorphism 35 22 97.105.19.61']\n",
    "logs_series = pd.Series(logs_list)\n",
    "logs_series.str.split(expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we meet the first requirement of tidy data. We have tabular data! \n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## One Value Per Cell\n",
    "\n",
    "The following table has multiple values in the City_State_Zip column. We will want to split that column into 3 different variables so that we can use one, all, some, or none of the variables in our analysis and modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe representing the untidy data\n",
    "\n",
    "untidy_df = pd.DataFrame({'Customer_ID': ['001', '002', '003'], \n",
    "                   'City_State_Zip': ['Dallas, TX, 75205', 'San Antonio, TX, 78209', 'Bend, OR, 97701']\n",
    "                  })\n",
    "\n",
    "untidy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the column into multiple columns, and name those columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the column into multiple columns\n",
    "new_vars = untidy_df.City_State_Zip.str.split(pat=',', expand=True)\n",
    "\n",
    "# rename the columns\n",
    "new_vars.columns = ['City', 'State', 'Zip']\n",
    "\n",
    "new_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to concatenate the new columns to the original dataframe and drop the original column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate new columns to the original dataframe\n",
    "untidy_df = pd.concat([untidy_df, new_vars], axis=1)\n",
    "\n",
    "# drop the original column\n",
    "tidy_df = untidy_df.drop(columns=['City_State_Zip'])\n",
    "\n",
    "tidy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we meet the second requirement of tidy data. We have one variable per column!\n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Each row is one and only one observation\n",
    "\n",
    "An observation is the level at which you want to analyze your data and/or make predictions. \n",
    "\n",
    "For example: \n",
    "- When making predictions on housing prices, an observation is a single property (property id). \n",
    "- When predicting whether or not a customer will churn, an observation is a customer (customer id). \n",
    "- When predicting student success in school, an observation is a student (student id). \n",
    "- When predicting whether a passenger will survive the titanic, an observation is a passenger (passenger id). \n",
    "- When analyzing customer reviews of products, an observation is a review (review id). \n",
    "\n",
    "When the data from an observation is scattered across multiple rows, then it is difficult to explore and impossible to model. \n",
    "\n",
    "Datasets that are untidy in this way are often referred to as **tall datasets**. \n",
    "\n",
    "Here is some *fictional* data on Codeup students. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'env' has no attribute 'user'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01menv\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_connection\u001b[39m(db, user\u001b[38;5;241m=\u001b[39m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser\u001b[49m, host\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mhost, password\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39mpassword):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmysql+pymysql://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassword\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      6\u001b[0m untidy_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_sql(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT * FROM students\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      7\u001b[0m                         env\u001b[38;5;241m.\u001b[39mget_connection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtidy_data\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'env' has no attribute 'user'"
     ]
    }
   ],
   "source": [
    "import env\n",
    "\n",
    "def get_connection(db, user=env.user, host=env.host, password=env.password):\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'\n",
    "    \n",
    "untidy_df = pd.read_sql('SELECT * FROM students', \n",
    "                        env.get_connection('tidy_data'))\n",
    "\n",
    "untidy_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the `var` column contains several different variable names, and the `val` column contains the values corresponding to each. Here we would be better off with seperate columns for `n_late_from_break`, `coffee_consumption`, and `classroom_temp` were seperate columns, as they are seperate variables.\n",
    "\n",
    "Using pandas, we can make this happen in a couple ways:\n",
    "\n",
    "- `pivot`\n",
    "- setting the index and `.unstack`ing\n",
    "\n",
    "We'll demonstrate both:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untidy_df.pivot(index='date', columns='var').head()\n",
    "\n",
    "# pivoting the df : opposite of 'melt'\n",
    "# pivoting the 'var' column and the values will be split into associated columns.\n",
    "\n",
    "# reset the index so that 'date' is not the index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not useful ? So we're told : untidy_df.set_index(['date', 'var']).unstack(level = 1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing this manipulation, pandas creates a `MultiIndex` for our rows and columns. In our case this isn't helping us out at all, so we can get rid of it to make working with the resulting data frame easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the var column into multiple variables\n",
    "tidy_df = untidy_df.pivot(index = 'date', columns = 'var')\n",
    "\n",
    "# drop the 'val' index level of the dataframe returned\n",
    "tidy_df.columns = tidy_df.columns.droplevel()\n",
    "\n",
    "# change the 'var' index-name given to the columns to an empty space\n",
    "tidy_df.columns.name = ''\n",
    "\n",
    "# reset the index so that date is in the body of the dataframe\n",
    "tidy_df = tidy_df.reset_index()\n",
    "tidy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tidy_cols(untidy_df, index_col):\n",
    "    \n",
    "    # split the var column into multiple variables\n",
    "    tidy_df = untidy_df.pivot(index = 'date', columns = 'var')\n",
    "\n",
    "    # drop the 'val' index level of the dataframe returned\n",
    "    tidy_df.columns = tidy_df.columns.droplevel()\n",
    "\n",
    "    # change the 'var' index-name given to the columns to an empty space\n",
    "    tidy_df.columns.name = ''\n",
    "\n",
    "    # reset the index so that date is in the body of the dataframe\n",
    "    tidy_df = tidy_df.reset_index()\n",
    "    \n",
    "    return tidy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tidy_cols(untidy_df, index_col = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we meet the third requirement of tidy data. Each observation is in one and only one row. \n",
    "\n",
    "<hr style=\"border:2px solid gray\">\n",
    "\n",
    "## Each variable is one and only one column\n",
    "\n",
    "When a single variable spans multiple columns, exploring data becomes very difficult. An example of a single variable spanning multiple columns are dummy variables. For example, when you created dummy variables from the titanic column of `embarked`, you created 3 new columns `embarked_southampton`, `embarked_cherbourg`, `embarked_queenstown`. This is an example of one variable spanning multiple columns. \n",
    "\n",
    "When you go into modeling and you have categorical variables, you will need to violate the component of \"each variable is one and only one column\" due to this need for dummy variables. But for exploration, you will want your data to be tidy. \n",
    "\n",
    "`pd.melt()` is the function to use when you need to tidy \"wide\" data like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untidy_sales = pd.read_sql('SELECT * FROM sales', env.sql_connexion('tidy_data'))\n",
    "\n",
    "untidy_sales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will need to \"melt\" this in two parts :**  \n",
    "First, we will _melt_ the sales.   \n",
    "Next, we will _melt_ the PPU data.  \n",
    "Finally, we will join the two dataframes together on product and year. We want our final data frame to have the columns product, year, sales, and ppu. \n",
    "\n",
    "A melt will combine multiple columns into two columns. There are 3 key parameters when melting:\n",
    "\n",
    "- `id_vars`: Which vars should *not* be melted. If omitted, all the columns in the data frame will be melted together.\n",
    "- `var_name`: The name of the column that will hold the names of the of the columns that will be combined.\n",
    "- `value_name`: The name of the column that will hold the resulting values.\n",
    "\n",
    "In our example above, the `id_vars` should be `product`, as we *don't* want to combine this column with any others. However, we will be combining the `2016 Sales`, `2017 Sales`, and `2018 Sales` columns into one. `var_name` will be `year` as \"year\" describes the column names of the columns we are combining, which will be the contents of the new column, and the `value_name` will be `sales`, as that is what the numerical values are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the sales columns\n",
    "\n",
    "sales_df = untidy_sales[['Product', '2016 Sales', '2017 Sales', '2018 Sales']]\n",
    "\n",
    "sales_df.head()\n",
    "\n",
    "# sales columns contain the values for each product (A, B, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns in the way we want it represented as categories in the \"year\" column. \n",
    "\n",
    "sales_df.columns = ['product', '2016', '2017', '2018']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt the data into 3 columns : product, year and sales\n",
    "\n",
    "sales_df = sales_df.melt(id_vars = ['product'], var_name = 'year', value_name = 'sales')\n",
    "\n",
    "sales_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the ppu columns\n",
    "\n",
    "ppu_df = untidy_sales[['Product', '2016 PPU', '2017 PPU', '2018 PPU']]\n",
    "\n",
    "\n",
    "# rename columns to be the years\n",
    "\n",
    "ppu_df.columns = ['product', '2016', '2017', '2018']\n",
    "\n",
    "\n",
    "# melt the data into 3 columns: product, year and ppu\n",
    "\n",
    "ppu_df = ppu_df.melt(id_vars = ['product'], var_name = 'year', value_name = 'ppu')\n",
    "\n",
    "\n",
    "# merge / join the sales_df and ppu_df on product and year\n",
    "# product & year are unique identifiers, and are both included to keep data legible.\n",
    "\n",
    "tidy_df = pd.merge(sales_df, ppu_df, on = ['product', 'year'])\n",
    "tidy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Hadley Wickham's Paper on Tidy Data](http://vita.had.co.nz/papers/tidy-data.html)\n",
    "- [pandas docs: reshaping and pivot tables](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Do your work for this exercise in a jupyter notebook or python script named `tidy_data`. Save this work in your `classification-exercises` repo. Add, commit, and push your changes.\n",
    "\n",
    "1. Attendance Data\n",
    "\n",
    "    Read the data from the `attendance` table and calculate an attendance percentage for each student. One half day is worth 50% of a full day, and 10 tardies is equal to one absence.\n",
    "    \n",
    "    You should end up with something like this:\n",
    "\n",
    "        name\n",
    "        Billy    0.5250\n",
    "        Jane     0.6875\n",
    "        John     0.9125\n",
    "        Sally    0.7625\n",
    "        Name: grade, dtype: float64\n",
    "\n",
    "2. Coffee Levels\n",
    "\n",
    "    1. Read the `coffee_levels` table.\n",
    "    1. Transform the data so that each carafe is in it's own column.\n",
    "    1. Is this the best shape for the data?\n",
    "\n",
    "3. Cake Recipes\n",
    "\n",
    "    1. Read the `cake_recipes` table. This data set contains cake tastiness scores for combinations of different recipes, oven rack positions, and oven temperatures.\n",
    "    1. Tidy the data as necessary.\n",
    "    1. Which recipe, on average, is the best? recipe b\n",
    "    1. Which oven temperature, on average, produces the best results? 275\n",
    "    1. Which combination of recipe, rack position, and temperature gives the best result? recipe b, bottom rack, 300 degrees\n",
    "\n",
    "4. **Bonus**: explore the other tables in the `tidy_data` database and reshape them as necessary so that they are in a tidy format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
